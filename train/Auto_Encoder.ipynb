{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fadc7574",
   "metadata": {},
   "source": [
    "# Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f5da7",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e63544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514a2c2",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b0dfca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "cfg = {\n",
    "    \"K\": 1024,\n",
    "    \"lr\" : 1e-4,\n",
    "    \"n_epochs\" : 500,\n",
    "    \"data_dir\" : \"./dataset\",\n",
    "    \"data_file\" : \"user_problem_mat.csv\",\n",
    "    \"batch_size\" : 64,\n",
    "    \"test_batch_size\" : 32,\n",
    "    \"topks\" : [10,20,50,100],\n",
    "}\n",
    "cfg[\"model_path\"] = f\"./saved_model/auto_encoder_K_{cfg['K']}_lr_{cfg['lr']}.pt\"\n",
    "cfg[\"device\"] = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f'Using {cfg[\"device\"]} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f8194",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f5f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "################# Test ##################\n",
    "#########################################\n",
    "\n",
    "def getLabel(test_data, pred_data):\n",
    "    r = []\n",
    "    for i in range(len(test_data)):\n",
    "        groundTrue = test_data[i]\n",
    "        predictTopK = pred_data[i]\n",
    "        pred = list(map(lambda x: x in groundTrue, predictTopK))\n",
    "        pred = np.array(pred).astype(\"float\")\n",
    "        r.append(pred)\n",
    "    return np.array(r).astype('float')\n",
    "\n",
    "def NDCGatK_r(test_data,r,k):\n",
    "    \"\"\"\n",
    "    Normalized Discounted Cumulative Gain\n",
    "    rel_i = 1 or 0, so 2^{rel_i} - 1 = 1 or 0\n",
    "    \"\"\"\n",
    "    assert len(r) == len(test_data)\n",
    "    pred_data = r[:, :k]\n",
    "\n",
    "    test_matrix = np.zeros((len(pred_data), k))\n",
    "    for i, items in enumerate(test_data):\n",
    "        length = k if k <= len(items) else len(items)\n",
    "        test_matrix[i, :length] = 1\n",
    "    max_r = test_matrix\n",
    "    idcg = np.sum(max_r * 1./np.log2(np.arange(2, k + 2)), axis=1)\n",
    "    dcg = pred_data*(1./np.log2(np.arange(2, k + 2)))\n",
    "    dcg = np.sum(dcg, axis=1)\n",
    "    idcg[idcg == 0.] = 1.\n",
    "    ndcg = dcg/idcg\n",
    "    ndcg[np.isnan(ndcg)] = 0.\n",
    "    return np.sum(ndcg)\n",
    "\n",
    "def RecallPrecision_ATk(test_data, r, k):\n",
    "    \"\"\"\n",
    "    test_data should be a list? cause users may have different amount of pos items. shape (test_batch, k)\n",
    "    pred_data : shape (test_batch, k) NOTE: pred_data should be pre-sorted\n",
    "    k : top-k\n",
    "    \"\"\"\n",
    "    right_pred = r[:, :k].sum(1)\n",
    "    precis_n = k\n",
    "    recall_n = np.array([len(test_data[i]) for i in range(len(test_data))])\n",
    "    recall = np.sum(right_pred/recall_n)\n",
    "    precis = np.sum(right_pred)/precis_n\n",
    "    return {'recall': recall, 'precision': precis}\n",
    "\n",
    "def test_one_batch(X, cfg):\n",
    "    sorted_items = X[0].numpy()\n",
    "    groundTrue = X[1]\n",
    "    r = getLabel(groundTrue, sorted_items)\n",
    "    pre, recall, ndcg = [], [], []\n",
    "    for k in cfg['topks']:\n",
    "        ret = RecallPrecision_ATk(groundTrue, r, k)\n",
    "        pre.append(ret['precision'])\n",
    "        recall.append(ret['recall'])\n",
    "        ndcg.append(NDCGatK_r(groundTrue,r,k))\n",
    "    return {'recall':np.array(recall), \n",
    "            'precision':np.array(pre), \n",
    "            'ndcg':np.array(ndcg)}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'}    \n",
    "    \n",
    "def add_to_user_problem_mat(idx, id, user_problem_mat : np.array):\n",
    "    data = requests.get(f'https://www.acmicpc.net/user/{id}', headers=headers)\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    trs = soup.select('div.problem-list')\n",
    "\n",
    "    for tr in trs:\n",
    "        problem_nums = tr.select('a')\n",
    "            \n",
    "        for problem_num in problem_nums :\n",
    "\n",
    "            problem_num = int(problem_num.text) - 1000\n",
    "            #print(problem_num)\n",
    "            try:\n",
    "                user_problem_mat[idx,problem_num] = 1\n",
    "            except:\n",
    "                print(\"범위를 벗어난 문제 번호 : \" + str(problem_num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1344d40",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf3a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "    def getUsersRating(self, user_row):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class AutoEncoder(BaseModel):\n",
    "    \n",
    "    def __init__(self, item_n, cfg):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        - sparse_matrix : user-item rating matrix\n",
    "        - cfg : configuration dict\n",
    "            - K (int)       : number of latent dimensions\n",
    "            - device : using device\n",
    "        \"\"\"\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # convert ndArray\n",
    "        self.item_n = item_n\n",
    "        self.K = cfg[\"K\"]\n",
    "        self.device = cfg[\"device\"]\n",
    "        \n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.I_1 = nn.Linear(self.item_n, self.K, bias=True, device = self.device)\n",
    "        self.I_2 = nn.Linear(self.K, self.item_n, bias=True, device = self.device)\n",
    "\n",
    "        nn.init.normal_(self.I_1.weight, std=1./self.K)\n",
    "        nn.init.normal_(self.I_2.weight, std=1./self.K)\n",
    "\n",
    "    def forward(self, x):\n",
    "        user_emb = self.I_1(x)\n",
    "        rating = self.I_2(user_emb)\n",
    " \n",
    "        return rating        \n",
    "\n",
    "    def getUsersRating(self, user_row : np.array):\n",
    "        return self.forward(torch.Tensor(user_row).cuda()).cpu().detach().numpy()\n",
    "\n",
    "class EASE():\n",
    "    \"\"\"\n",
    "    Embarrassingly Shallow Autoencoders model class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambda_):\n",
    "        self.B = None\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def train(self, interaction_matrix):\n",
    "        \"\"\"\n",
    "        train pass\n",
    "        :param interaction_matrix: interaction_matrix\n",
    "        \"\"\"\n",
    "        G = interaction_matrix.T @ interaction_matrix\n",
    "        diag = list(range(G.shape[0]))\n",
    "        G[diag, diag] += self.lambda_\n",
    "        P = np.linalg.inv(G)\n",
    "\n",
    "        # B = P * (X^T * X − diagMat(γ))\n",
    "        self.B = P / -np.diag(P)\n",
    "        min_dim = min(*self.B.shape)\n",
    "        self.B[range(min_dim), range(min_dim)] = 0\n",
    "\n",
    "    def forward(self, user_row):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        \"\"\"\n",
    "        return user_row @ self.B\n",
    "    \n",
    "    def getUsersRating(self, user_row : np.array):\n",
    "        return self.forward(user_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984d642",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "627d25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 상속\n",
    "class AEDataset(Dataset): \n",
    "    def __init__(self, sparse_matrix, test=False):\n",
    "        self.sparse_matrix = sparse_matrix.fillna(0).to_numpy()\n",
    "        \n",
    "        if test:\n",
    "            self.maxK = max(cfg['topks'])\n",
    "            test_data = []\n",
    "            \n",
    "            for user in range(len(self.sparse_matrix)):      \n",
    "                items = self.sparse_matrix[user].nonzero()[0]\n",
    "                if len(items) >= self.maxK :\n",
    "                    test_data.append(self.sparse_matrix[user])  \n",
    "                    \n",
    "            self.sparse_matrix = np.array(test_data)    \n",
    "            print(\"complete making test dict\")      \n",
    "\n",
    "    # 총 데이터의 개수를 리턴\n",
    "    def __len__(self): \n",
    "        return len(self.sparse_matrix)\n",
    "\n",
    "    # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx): \n",
    "        x = torch.FloatTensor(self.sparse_matrix[idx]).cuda()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f084c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = f'{cfg[\"data_dir\"]}/train_{cfg[\"data_file\"]}'\n",
    "train_s_mat = pd.read_csv(train_data_path, index_col = 0)\n",
    "\n",
    "test_data_path = f'{cfg[\"data_dir\"]}/test_{cfg[\"data_file\"]}'\n",
    "test_s_mat = pd.read_csv(test_data_path, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8fc243f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete making test dict\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AEDataset(train_s_mat)\n",
    "test_dataset = AEDataset(test_s_mat, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1ce7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=cfg[\"batch_size\"])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=cfg[\"test_batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d5305",
   "metadata": {},
   "source": [
    "## Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86b6923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, X)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def make_test_list(data):\n",
    "    test_list = []\n",
    "    for d in data:\n",
    "        test_list.append(d.nonzero().squeeze(1))\n",
    "    return test_list\n",
    "\n",
    "def test_loop(dataloader, model, cfg, loss_fn=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    model.eval()\n",
    "    max_K = max(cfg['topks'])\n",
    "    \n",
    "    results = {'precision': np.zeros(len(cfg['topks'])),\n",
    "               'recall': np.zeros(len(cfg['topks'])),\n",
    "               'ndcg': np.zeros(len(cfg['topks'])),\n",
    "               'test_loss' : 0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            assert num_batches <= size / 10\n",
    "        except AssertionError:\n",
    "            print(f\"test_u_batch_size is too big for this dataset, try a small one {size // 10}\")\n",
    "        rating_list = []\n",
    "        groundTrue_list = []        \n",
    "        for X in dataloader:\n",
    "            rating = model(X)\n",
    "            _, rating_K = torch.topk(rating, k=max_K) # rating_K는 rating에서 k번째 높은 rating까지의 index\n",
    "            \n",
    "            rating_list.append(rating_K.cpu())\n",
    "            gt = make_test_list(X)\n",
    "            groundTrue_list.append(gt)\n",
    "    \n",
    "            if loss_fn is not None:\n",
    "                results['test_loss'] += loss_fn(rating, X).item()\n",
    "            del rating\n",
    "\n",
    "        X = zip(rating_list, groundTrue_list)\n",
    "        pre_results = []\n",
    "        for x in X:\n",
    "            pre_results.append(test_one_batch(x,cfg))\n",
    "        for result in pre_results:\n",
    "            results['recall'] += result['recall']\n",
    "            results['precision'] += result['precision']\n",
    "            results['ndcg'] += result['ndcg']\n",
    "        results['recall'] /= float(size)\n",
    "        results['precision'] /= float(size)\n",
    "        results['ndcg'] /= float(size)\n",
    "            \n",
    "    results['test_loss'] /= num_batches\n",
    "    return results\n",
    "    \n",
    "def test_loop_np(dataloader, model, cfg, loss_fn=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    max_K = max(cfg['topks'])\n",
    "    \n",
    "    results = {'precision': np.zeros(len(cfg['topks'])),\n",
    "               'recall': np.zeros(len(cfg['topks'])),\n",
    "               'ndcg': np.zeros(len(cfg['topks'])),\n",
    "               'test_loss' : 0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            assert num_batches <= size / 10\n",
    "        except AssertionError:\n",
    "            print(f\"test_u_batch_size is too big for this dataset, try a small one {size // 10}\")\n",
    "        rating_list = []\n",
    "        groundTrue_list = []        \n",
    "        for X in dataloader:\n",
    "            rating = torch.tensor(model.getUsersRating(X.cpu().numpy())).cuda()\n",
    "            _, rating_K = torch.topk(rating, k=max_K) # rating_K는 rating에서 k번째 높은 rating까지의 index\n",
    "            \n",
    "            rating_list.append(rating_K.cpu())\n",
    "            gt = make_test_list(X)\n",
    "            groundTrue_list.append(gt)\n",
    "    \n",
    "            if loss_fn is not None:\n",
    "                results['test_loss'] += loss_fn(rating, X).item()\n",
    "            del rating\n",
    "\n",
    "        X = zip(rating_list, groundTrue_list)\n",
    "        pre_results = []\n",
    "        for x in X:\n",
    "            pre_results.append(test_one_batch(x,cfg))\n",
    "        for result in pre_results:\n",
    "            results['recall'] += result['recall']\n",
    "            results['precision'] += result['precision']\n",
    "            results['ndcg'] += result['ndcg']\n",
    "        results['recall'] /= float(size)\n",
    "        results['precision'] /= float(size)\n",
    "        results['ndcg'] /= float(size)\n",
    "            \n",
    "    results['test_loss'] /= num_batches\n",
    "    return results\n",
    "\n",
    "def save_result_to_tensor_board(result, epoch, div=\"Test\"):\n",
    "    for i, k in enumerate(cfg[\"topks\"]):\n",
    "        writer.add_scalar(f\"[{div}]/Precision_@{k}\", result['precision'][i], epoch)\n",
    "        writer.add_scalar(f\"[{div}]/Recall_@{k}\", result['recall'][i], epoch)\n",
    "        writer.add_scalar(f\"[{div}]/ndcg_@{k}\", result['ndcg'][i], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3ea36034",
   "metadata": {},
   "outputs": [],
   "source": [
    "RecModel = AutoEncoder(train_s_mat.shape[1], cfg)\n",
    "optim = torch.optim.Adam(RecModel.parameters(), lr=cfg[\"lr\"], betas=(0.9, 0.999), weight_decay=1e-6)\n",
    "loss_fn = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "#torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "2709d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_recall = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3aa0486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST [train_data]\n",
      "{'precision': array([0.00417143, 0.00405714, 0.00426286, 0.00545714]), 'recall': array([0.00014064, 0.00027028, 0.00069549, 0.00175348]), 'ndcg': array([0.00395303, 0.00393439, 0.00414827, 0.00511296]), 'test_loss': 0.010870212841440331}\n",
      "TEST [test_data]\n",
      "{'precision': array([0.00453333, 0.00366667, 0.00437333, 0.00532   ]), 'recall': array([0.00015044, 0.00024111, 0.00070448, 0.00177643]), 'ndcg': array([0.00429021, 0.00373487, 0.00422753, 0.00501043]), 'test_loss': 0.010464023620365782}\n",
      "==========================\n",
      "loss: 0.007082  [    0/  500]\n",
      "loss: 0.005522  [    5/  500]\n",
      "loss: 0.004476  [   10/  500]\n",
      "loss: 0.003583  [   15/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([0.99845714, 0.99685714, 0.99113143, 0.97121143]), 'recall': array([0.03938244, 0.07861227, 0.19514111, 0.38068008]), 'ndcg': array([0.99878006, 0.99755435, 0.99306297, 0.97733818]), 'test_loss': 0.003034477871419354}\n",
      "TEST [test_data]\n",
      "{'precision': array([0.9936    , 0.9912    , 0.97976   , 0.94942667]), 'recall': array([0.04034547, 0.08046286, 0.19850015, 0.38306291]), 'ndcg': array([0.99415209, 0.99226147, 0.98336826, 0.95911466]), 'test_loss': 0.0036757425355546654}\n",
      "==========================\n",
      "loss: 0.002838  [   20/  500]\n",
      "loss: 0.002324  [   25/  500]\n",
      "loss: 0.001964  [   30/  500]\n",
      "loss: 0.001705  [   35/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([0.99994286, 0.99977143, 0.99931429, 0.99517714]), 'recall': array([0.03944834, 0.07888042, 0.19708179, 0.39202572]), 'ndcg': array([0.99996214, 0.99983447, 0.9994817 , 0.99632334]), 'test_loss': 0.0019384894117882305}\n",
      "TEST [test_data]\n",
      "{'precision': array([0.99893333, 0.99726667, 0.99141333, 0.973     ]), 'recall': array([0.04057822, 0.08100851, 0.20116984, 0.3938463 ]), 'ndcg': array([0.99923946, 0.99798291, 0.99338463, 0.97873873]), 'test_loss': 0.0031560742930370443}\n",
      "==========================\n",
      "loss: 0.001450  [   40/  500]\n",
      "loss: 0.001260  [   45/  500]\n",
      "loss: 0.001094  [   50/  500]\n",
      "loss: 0.000970  [   55/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1.        , 1.        , 0.99996571, 0.99941714]), 'recall': array([0.03945172, 0.07890343, 0.19724871, 0.39419994]), 'ndcg': array([1.        , 1.        , 0.99997453, 0.99956264]), 'test_loss': 0.001446233790325509}\n",
      "TEST [test_data]\n",
      "{'precision': array([0.99986667, 0.99933333, 0.99589333, 0.98402667]), 'recall': array([0.04061859, 0.08119565, 0.20221412, 0.39894208]), 'ndcg': array([0.99990743, 0.9995204 , 0.99691271, 0.98758482]), 'test_loss': 0.002886971493469591}\n",
      "==========================\n",
      "loss: 0.000905  [   60/  500]\n",
      "loss: 0.000797  [   65/  500]\n",
      "loss: 0.000715  [   70/  500]\n",
      "loss: 0.000649  [   75/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1.     , 1.     , 1.     , 0.99996]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39449409]), 'ndcg': array([1.        , 1.        , 1.        , 0.99997081]), 'test_loss': 0.0009891154805452308}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 0.9998    , 0.99909333, 0.99349333]), 'recall': array([0.04062342, 0.08123339, 0.20291075, 0.40320704]), 'ndcg': array([1.        , 0.99985866, 0.99931554, 0.99501639]), 'test_loss': 0.0025322118432241233}\n",
      "==========================\n",
      "loss: 0.000598  [   80/  500]\n",
      "loss: 0.000559  [   85/  500]\n",
      "loss: 0.000526  [   90/  500]\n",
      "loss: 0.000512  [   95/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.0007939923374744301}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 0.99973333, 0.99712   ]), 'recall': array([0.04062342, 0.08124685, 0.20306536, 0.40486962]), 'ndcg': array([1.        , 1.        , 0.99980181, 0.99781907]), 'test_loss': 0.00235087084683015}\n",
      "==========================\n",
      "loss: 0.000470  [  100/  500]\n",
      "loss: 0.000436  [  105/  500]\n",
      "loss: 0.000415  [  110/  500]\n",
      "loss: 0.000400  [  115/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.0006523711755025116}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 0.99984   , 0.99857333]), 'recall': array([0.04062342, 0.08124685, 0.20308613, 0.40555247]), 'ndcg': array([1.        , 1.        , 0.99988474, 0.99892663]), 'test_loss': 0.002200132191300075}\n",
      "==========================\n",
      "loss: 0.000388  [  120/  500]\n",
      "loss: 0.000376  [  125/  500]\n",
      "loss: 0.000352  [  130/  500]\n",
      "loss: 0.000336  [  135/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.0005624256292569705}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 0.99992   , 0.99913333]), 'recall': array([0.04062342, 0.08124685, 0.20310264, 0.40581934]), 'ndcg': array([1.        , 1.        , 0.99994199, 0.99935363]), 'test_loss': 0.002094809919536272}\n",
      "==========================\n",
      "loss: 0.000331  [  140/  500]\n",
      "loss: 0.000321  [  145/  500]\n",
      "loss: 0.000313  [  150/  500]\n",
      "loss: 0.000314  [  155/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.0005049762670585716}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 0.99994667, 0.99952   ]), 'recall': array([0.04062342, 0.08124685, 0.20310552, 0.40599645]), 'ndcg': array([1.        , 1.        , 0.99996113, 0.99963997]), 'test_loss': 0.0020203186060361407}\n",
      "==========================\n",
      "loss: 0.000299  [  160/  500]\n",
      "loss: 0.000291  [  165/  500]\n",
      "loss: 0.000291  [  170/  500]\n",
      "loss: 0.000280  [  175/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.000469689605101435}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 0.99994667, 0.99961333]), 'recall': array([0.04062342, 0.08124685, 0.20310552, 0.40604188]), 'ndcg': array([1.        , 1.        , 0.99996267, 0.99971121]), 'test_loss': 0.0019689284071841453}\n",
      "==========================\n",
      "loss: 0.000277  [  180/  500]\n",
      "loss: 0.000270  [  185/  500]\n",
      "loss: 0.000267  [  190/  500]\n",
      "loss: 0.000262  [  195/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.00044057775336444715}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 0.99997333, 0.99968   ]), 'recall': array([0.04062342, 0.08124685, 0.20311035, 0.40607454]), 'ndcg': array([1.        , 1.        , 0.99998168, 0.99976132]), 'test_loss': 0.0019273115470609132}\n",
      "==========================\n",
      "loss: 0.000262  [  200/  500]\n",
      "loss: 0.000254  [  205/  500]\n",
      "loss: 0.000255  [  210/  500]\n",
      "loss: 0.000251  [  215/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.0004183308335169303}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.     , 1.     , 1.     , 0.99976]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.4061128 ]), 'ndcg': array([1.        , 1.        , 1.        , 0.99982043]), 'test_loss': 0.0018948563126886778}\n",
      "==========================\n",
      "loss: 0.000249  [  220/  500]\n",
      "loss: 0.000249  [  225/  500]\n",
      "loss: 0.000242  [  230/  500]\n",
      "loss: 0.000243  [  235/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.000402263256943446}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 1.        , 0.99977333]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40611383]), 'ndcg': array([1.        , 1.        , 1.        , 0.99983102]), 'test_loss': 0.001870684801264012}\n",
      "==========================\n",
      "loss: 0.000240  [  240/  500]\n",
      "loss: 0.000236  [  245/  500]\n",
      "loss: 0.000235  [  250/  500]\n",
      "loss: 0.000231  [  255/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.0003964465206742964}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 1.        , 0.99978667]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40612104]), 'ndcg': array([1.        , 1.        , 1.        , 0.99984137]), 'test_loss': 0.0018573922224025777}\n",
      "==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.000238  [  260/  500]\n",
      "loss: 0.000226  [  265/  500]\n",
      "loss: 0.000224  [  270/  500]\n",
      "loss: 0.000225  [  275/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.0003778457943223078}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 1.        , 0.99981333]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40613336]), 'ndcg': array([1.        , 1.        , 1.        , 0.99986059]), 'test_loss': 0.0018347507278296225}\n",
      "==========================\n",
      "loss: 0.000228  [  280/  500]\n",
      "loss: 0.000224  [  285/  500]\n",
      "loss: 0.000223  [  290/  500]\n",
      "loss: 0.000223  [  295/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.00037137606149454684}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 1.        , 0.99981333]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40613336]), 'ndcg': array([1.        , 1.        , 1.        , 0.99986096]), 'test_loss': 0.001823445155404191}\n",
      "==========================\n",
      "loss: 0.000219  [  300/  500]\n",
      "loss: 0.000219  [  305/  500]\n",
      "loss: 0.000225  [  310/  500]\n",
      "loss: 0.000217  [  315/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.00036366032251283867}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 1.        , 0.99982667]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40614121]), 'ndcg': array([1.       , 1.       , 1.       , 0.9998705]), 'test_loss': 0.00181252574746279}\n",
      "==========================\n",
      "loss: 0.000218  [  320/  500]\n",
      "loss: 0.000217  [  325/  500]\n",
      "loss: 0.000216  [  330/  500]\n",
      "loss: 0.000215  [  335/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.0003592561843106523}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.     , 1.     , 1.     , 0.99984]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40614841]), 'ndcg': array([1.        , 1.        , 1.        , 0.99988026]), 'test_loss': 0.0018049405684615386}\n",
      "==========================\n",
      "loss: 0.000211  [  340/  500]\n",
      "loss: 0.000218  [  345/  500]\n",
      "loss: 0.000211  [  350/  500]\n",
      "loss: 0.000210  [  355/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.0003555836412653496}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.     , 1.     , 1.     , 0.99984]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40614841]), 'ndcg': array([1.        , 1.        , 1.        , 0.99988024]), 'test_loss': 0.0017985863427809896}\n",
      "==========================\n",
      "loss: 0.000212  [  360/  500]\n",
      "loss: 0.000211  [  365/  500]\n",
      "loss: 0.000211  [  370/  500]\n",
      "loss: 0.000213  [  375/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.00035109169971705837}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.     , 1.     , 1.     , 0.99984]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40614841]), 'ndcg': array([1.       , 1.       , 1.       , 0.9998805]), 'test_loss': 0.00179269667120373}\n",
      "==========================\n",
      "loss: 0.000209  [  380/  500]\n",
      "loss: 0.000207  [  385/  500]\n",
      "loss: 0.000210  [  390/  500]\n",
      "loss: 0.000209  [  395/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.00034991552952719343}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.     , 1.     , 1.     , 0.99984]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40614841]), 'ndcg': array([1.        , 1.        , 1.        , 0.99988057]), 'test_loss': 0.0017894890303029975}\n",
      "==========================\n",
      "loss: 0.000206  [  400/  500]\n",
      "loss: 0.000207  [  405/  500]\n",
      "loss: 0.000209  [  410/  500]\n",
      "loss: 0.000206  [  415/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.00034751457834235306}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.     , 1.     , 1.     , 0.99984]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40614841]), 'ndcg': array([1.        , 1.        , 1.        , 0.99988068]), 'test_loss': 0.0017858040593366356}\n",
      "==========================\n",
      "loss: 0.000207  [  420/  500]\n",
      "loss: 0.000204  [  425/  500]\n",
      "loss: 0.000204  [  430/  500]\n",
      "loss: 0.000206  [  435/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.0003454233072592284}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.     , 1.     , 1.     , 0.99984]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40614841]), 'ndcg': array([1.       , 1.       , 1.       , 0.9998807]), 'test_loss': 0.0017822567532037167}\n",
      "==========================\n",
      "loss: 0.000208  [  440/  500]\n",
      "loss: 0.000206  [  445/  500]\n",
      "loss: 0.000205  [  450/  500]\n",
      "loss: 0.000206  [  455/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.00034483652958773413}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.     , 1.     , 1.     , 0.99984]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.40614841]), 'ndcg': array([1.        , 1.        , 1.        , 0.99988057]), 'test_loss': 0.00178081317101308}\n",
      "==========================\n",
      "loss: 0.000206  [  460/  500]\n",
      "loss: 0.000204  [  465/  500]\n",
      "loss: 0.000206  [  470/  500]\n",
      "loss: 0.000206  [  475/  500]\n",
      "TEST [train_data]\n",
      "{'precision': array([1., 1., 1., 1.]), 'recall': array([0.03945172, 0.07890343, 0.19725858, 0.39451717]), 'ndcg': array([1., 1., 1., 1.]), 'test_loss': 0.0003427393520145084}\n",
      "TEST [test_data]\n",
      "{'precision': array([1.        , 1.        , 1.        , 0.99985333]), 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.4061518 ]), 'ndcg': array([1.        , 1.        , 1.        , 0.99989041]), 'test_loss': 0.0017784015681753134}\n",
      "==========================\n",
      "loss: 0.000205  [  480/  500]\n",
      "loss: 0.000203  [  485/  500]\n",
      "loss: 0.000204  [  490/  500]\n",
      "loss: 0.000203  [  495/  500]\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "st_num = 0\n",
    "\n",
    "for i in range(st_num,cfg[\"n_epochs\"]+st_num):\n",
    "    if i % 20 == 0:\n",
    "        train_res = test_loop(train_dataloader, RecModel, cfg, loss_fn)\n",
    "        test_res = test_loop(test_dataloader, RecModel, cfg, loss_fn)\n",
    "        save_result_to_tensor_board(train_res, i, div=\"Train\")\n",
    "        save_result_to_tensor_board(test_res, i)    \n",
    "        writer.add_scalar(f\"Loss/test\", test_res['test_loss'], i)\n",
    "        print(\"TEST [train_data]\")\n",
    "        print(train_res)\n",
    "        print(\"TEST [test_data]\")\n",
    "        print(test_res)\n",
    "        print(\"==========================\")\n",
    "        if max_recall < max(test_res[\"recall\"]) :\n",
    "            max_recall = max(test_res[\"recall\"])\n",
    "            torch.save(RecModel.state_dict(), cfg[\"model_path\"])\n",
    "    loss = train_loop(train_dataloader, RecModel, loss_fn, optim)  \n",
    "    writer.add_scalar(f\"Loss/train\", loss, i)\n",
    "    if i % 5 == 0:\n",
    "        print(f\"loss: {loss:>7f}  [{i:>5d}/{cfg['n_epochs']+st_num:>5d}]\")\n",
    "            \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1352d315",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m RecModel \u001b[38;5;241m=\u001b[39m AutoEncoder(num_problem, cfg)\n\u001b[0;32m      7\u001b[0m RecModel\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtest_loop\u001b[49m(test_dataloader, RecModel, loss_fn, cfg)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loop' is not defined"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "RecModel = AutoEncoder(num_problem, cfg)\n",
    "RecModel.load_state_dict(torch.load(cfg[\"model_path\"]))\n",
    "test_loop(test_dataloader, RecModel, cfg, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bc00325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "st_pb_num = 1000\n",
    "ed_pb_num = 27981\n",
    "num_problem = ed_pb_num - st_pb_num + 1  # 1000 ~ 27981\n",
    "\n",
    "RecModel = AutoEncoder(num_problem, cfg)\n",
    "RecModel.load_state_dict(torch.load(cfg[\"model_path\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e1d3119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete making test dict\n"
     ]
    }
   ],
   "source": [
    "test_data_path = f'{cfg[\"data_dir\"]}/test_{cfg[\"data_file\"]}'\n",
    "test_s_mat = pd.read_csv(test_data_path, index_col = 0)\n",
    "test_dataset = AEDataset(test_s_mat, test=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=cfg[\"test_batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9026eba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': array([1.        , 1.        , 1.        , 0.99985333]),\n",
       " 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.4061518 ]),\n",
       " 'ndcg': array([1.        , 1.        , 1.        , 0.99989041]),\n",
       " 'test_loss': 0.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loop(test_dataloader, RecModel, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738852eb",
   "metadata": {},
   "source": [
    "# Tensorboard 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb17f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f5ee0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e87687b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 288), started 1 day, 1:58:31 ago. (Use '!kill 288' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-478ef19fb3e3d159\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-478ef19fb3e3d159\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe6e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard dev upload --logdir runs \\\n",
    "--name \"My latest experiment\" \\ # 선택 사항\n",
    "--description \"Simple comparison of several hyperparameters\" # 선택 사항"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d42db",
   "metadata": {},
   "source": [
    "# 유저에 따른 문제 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "43e53b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MFGDTorch(\n",
       "  (I_1): Linear(in_features=26982, out_features=128, bias=True)\n",
       "  (I_2): Linear(in_features=128, out_features=26982, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "RecModel = AutoEncoder(train_s_mat, cfg)\n",
    "RecModel.load_state_dict(torch.load(cfg[\"model_path\"]))\n",
    "RecModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "06556742",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_pb_num = 1000\n",
    "ed_pb_num = 27981\n",
    "num_problem = ed_pb_num - st_pb_num + 1  # 1000 ~ 27981\n",
    "\n",
    "NUM_TOP_PROBLEMS = 10\n",
    "user_id = 'faang12594'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aae23a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범위를 벗어난 문제 번호 : 26982\n",
      "범위를 벗어난 문제 번호 : 26983\n"
     ]
    }
   ],
   "source": [
    "user_problem = np.zeros([1, num_problem])\n",
    "add_to_user_problem_mat(0, user_id, user_problem)\n",
    "\n",
    "result = RecModel.getUsersRating(user_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d655f00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16234 11657  1707  1976 17609 21758  1655  4149  5615  2436]]\n"
     ]
    }
   ],
   "source": [
    "# 유저가 푼 문제와 비슷한 유형 추천 - 예전\n",
    "result[user_problem.nonzero()] = -np.inf\n",
    "top_problems_by_user = bn.argpartition(-result, NUM_TOP_PROBLEMS, axis=1)[:, :NUM_TOP_PROBLEMS] # 값이 큰 10개 문제 고름\n",
    "top_problems_by_user += 1000\n",
    "print(top_problems_by_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b3961",
   "metadata": {},
   "source": [
    "# 모델 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86eda47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MFGDTorch(\n",
       "  (I_1): Linear(in_features=26982, out_features=128, bias=True)\n",
       "  (I_2): Linear(in_features=128, out_features=26982, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 모델 불러오기\n",
    "with open('./saved_model/ease_model_1682656807.5593908.p', 'rb') as file:\n",
    "    ease = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "76a00830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae = AutoEncoder(train_s_mat, cfg)\n",
    "ae.load_state_dict(torch.load(cfg[\"model_path\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2ede01fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - AE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': array([1.        , 1.        , 1.        , 0.99985333]),\n",
       " 'recall': array([0.04062342, 0.08124685, 0.20311712, 0.4061518 ]),\n",
       " 'ndcg': array([1.        , 1.        , 1.        , 0.99989041]),\n",
       " 'test_loss': 0.0017784015681753134}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test - AE\")\n",
    "test_loop(test_dataloader, ae, loss_fn, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2fdd0012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - EASE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': array([0.9904    , 0.98726667, 0.97461333, 0.9416    ]),\n",
       " 'recall': array([0.04021442, 0.08015774, 0.19754144, 0.38002484]),\n",
       " 'ndcg': array([0.99190783, 0.98921548, 0.97900032, 0.9523618 ]),\n",
       " 'test_loss': 0.00385346341856047}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test - EASE\")\n",
    "test_loop_np(test_dataloader, ease, loss_fn, cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
