{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fadc7574",
   "metadata": {},
   "source": [
    "# Light_GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f5da7",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e63544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import time \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514a2c2",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0dfca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'batch_size' : 4096,\n",
    "    'test_batch_size' : 32,\n",
    "    'dataset_dir_path' : \"./dataset/lgcn\",\n",
    "    'dataset_file_name' : \"user_problem_mat.csv\",\n",
    "    'adj_mat_name' : \"s_pre_adj_mat.npz\",\n",
    "    'd_mat_name' : \"d_mat.npz\",\n",
    "    'embedding_dim' : 10,\n",
    "    'n_layers' : 3,\n",
    "    'pretrain' : False,\n",
    "    'dropout' : False,\n",
    "    'A_split' : False,\n",
    "    'device' : 'cuda',\n",
    "    'decay' : 1e-4,\n",
    "    'lr' : 1e-3,\n",
    "    'model_file_path' : './saved_model/lgcn_weight_10.pt',\n",
    "    'topks' : [10,20],\n",
    "}\n",
    "\n",
    "if cfg['pretrain']:\n",
    "    try:\n",
    "        model = torch.load(cfg['model_file_path'])\n",
    "        cfg['user_emb'] = model['embedding_user.weight']\n",
    "        cfg['item_emb'] = model['embedding_item.weight']\n",
    "    except:\n",
    "        print('there is no trained weight')\n",
    "        cfg['pretrain'] = False\n",
    "\n",
    "if cfg['dropout']:\n",
    "    cfg['keep_prob'] = 0.3\n",
    "\n",
    "if cfg['A_split']:\n",
    "    cfg['A_fold'] = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984d642",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627d25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        print(\"init dataset\")\n",
    "    \n",
    "    @property\n",
    "    def n_users(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def allPos(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def train_data_size(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def testDict(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserPosItems(self, users):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserNegItems(self, users):\n",
    "        \"\"\"\n",
    "        not necessary for large dataset\n",
    "        it's stupid to return all neg items in super large dataset\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getSparseGraph(self):\n",
    "        \"\"\"\n",
    "        build a graph in torch.sparse.IntTensor.\n",
    "        Details in NGCF's matrix form\n",
    "        A = \n",
    "            |I,   R|\n",
    "            |R^T, I|\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BOJDataSet(BasicDataset):\n",
    "    def __init__(self, config):\n",
    "        print(\"init dataset\")\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.path = config['dataset_dir_path']\n",
    "        self.adj_mat_name = config['adj_mat_name']\n",
    "        self.d_mat_name = config['d_mat_name']\n",
    "\n",
    "        train_path = self.path+'/'+ \"train_\" + config['dataset_file_name']\n",
    "        test_path = self.path+'/'+ \"test_\" + config['dataset_file_name']\n",
    "        self.train_user_item_mat = pd.read_csv(train_path, index_col = 0).to_numpy()\n",
    "        self.test_user_item_mat = pd.read_csv(test_path, index_col = 0).to_numpy()\n",
    "                        \n",
    "        self.nz_train_user_item = self.train_user_item_mat.nonzero()\n",
    "        self._train_data_size = self.nz_train_user_item[0].shape[0]\n",
    "        self.n_user = self.train_user_item_mat.shape[0]\n",
    "        self.m_item = self.train_user_item_mat.shape[1]\n",
    "        \n",
    "        self.split = config['A_split']\n",
    "        if self.split:\n",
    "            self.folds = config['A_fold']\n",
    "        self.device = config['device']        \n",
    "        \n",
    "        print('make testDict')\n",
    "        self.maxK = max(config['topks'])\n",
    "        self.n_test_user = self.test_user_item_mat.shape[0]\n",
    "        \n",
    "        self._allPos = self.getUserPosItems(list(range(self.n_user)))\n",
    "        self.__testDict = self.__build_test()\n",
    "        self.__trainDict = self.__build_test(test=False)\n",
    "\n",
    "        print(f\"train data size : {self._train_data_size}\")\n",
    "        print(f\"test user size : {self.n_test_user}\")\n",
    "        \n",
    "        self.Graph = None\n",
    "        \n",
    "    @property\n",
    "    def n_users(self):\n",
    "        return self.n_user\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        return self.m_item\n",
    "    \n",
    "    @property\n",
    "    def train_data_size(self):\n",
    "        return self._train_data_size\n",
    "    \n",
    "    @property\n",
    "    def allPos(self):\n",
    "        return self._allPos\n",
    "\n",
    "    @property\n",
    "    def testDict(self):\n",
    "        return self.__testDict\n",
    "    \n",
    "    @property\n",
    "    def trainDict(self):\n",
    "        return self.__trainDict\n",
    "    \n",
    "    # 유저가 푼 문제번호 반환\n",
    "    def getUserPosItems(self, users):\n",
    "        posItems = []\n",
    "        for user in users:\n",
    "            posItems.append(self.train_user_item_mat[user].nonzero()[0])\n",
    "        return posItems\n",
    "        \n",
    "    def _split_A_hat(self,A):\n",
    "        A_fold = []\n",
    "        fold_len = (self.n_users + self.m_items) // self.folds\n",
    "        for i_fold in range(self.folds):\n",
    "            start = i_fold*fold_len\n",
    "            if i_fold == self.folds - 1:\n",
    "                end = self.n_users + self.m_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "            A_fold.append(self._convert_sp_mat_to_sp_tensor(A[start:end]).coalesce().to(self.device))\n",
    "        return A_fold\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        row = torch.Tensor(coo.row).long()\n",
    "        col = torch.Tensor(coo.col).long()\n",
    "        index = torch.stack([row, col])\n",
    "        data = torch.FloatTensor(coo.data)\n",
    "        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n",
    "        \n",
    "    def getSparseGraph(self):\n",
    "        print(\"loading adjacency matrix\")\n",
    "        if self.Graph is None:\n",
    "            try:\n",
    "                pre_adj_mat = sp.load_npz(self.path + '/'+self.adj_mat_name)\n",
    "                print(\"successfully loaded...\")\n",
    "                self.d_mat = sp.load_npz(self.path + '/'+self.d_mat_name)\n",
    "                norm_adj = pre_adj_mat\n",
    "            except :\n",
    "                print(\"generating adjacency matrix\")\n",
    "                s = time.time()\n",
    "                adj_mat = sp.dok_matrix((self.n_users + self.m_items, self.n_users + self.m_items), dtype=np.float32)\n",
    "                adj_mat = adj_mat.tolil()\n",
    "                R = self.train_user_item_mat\n",
    "                adj_mat[:self.n_users, self.n_users:] = R\n",
    "                adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "                adj_mat = adj_mat.todok()\n",
    "                # adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n",
    "                \n",
    "                rowsum = np.array(adj_mat.sum(axis=1))\n",
    "                d_inv = np.power(rowsum, -0.5).flatten()\n",
    "                d_inv[np.isinf(d_inv)] = 0.\n",
    "                d_mat = sp.diags(d_inv)\n",
    "                \n",
    "                # 평가를 위한 mat 저장      \n",
    "                self.d_mat = d_mat\n",
    "                sp.save_npz(self.path + '/'+self.d_mat_name, self.d_mat)\n",
    "                \n",
    "                norm_adj = d_mat.dot(adj_mat) # 유저가 시도한 문제 개수 제곱근으로 나눠주기\n",
    "                norm_adj = norm_adj.dot(d_mat) # 문제가 시도되어진 개수 제곱근으로 나눠주기\n",
    "                norm_adj = norm_adj.tocsr()\n",
    "                end = time.time()\n",
    "                print(f\"costing {end-s}s, saved norm_mat...\")\n",
    "                sp.save_npz(self.path + '/'+self.adj_mat_name, norm_adj)\n",
    "\n",
    "            if self.split == True:\n",
    "                self.Graph = self._split_A_hat(norm_adj)\n",
    "                print(\"done split matrix\")\n",
    "            else:\n",
    "                self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "                self.Graph = self.Graph.coalesce().to(self.device)\n",
    "                print(\"don't split the matrix\")\n",
    "        return self.Graph\n",
    "\n",
    "    def getEvalSparseGraph(self, users):\n",
    "        graph = None\n",
    "        adj_mat = sp.dok_matrix((len(users), self.n_users + self.m_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        adj_mat[:, self.n_users:] = self.test_user_item_mat[users]\n",
    "        adj_mat = adj_mat.todok()                \n",
    "        \n",
    "        rowsum = np.array(adj_mat.sum(axis=1))\n",
    "        d_inv = np.power(rowsum, -0.5).flatten()\n",
    "        d_inv[np.isinf(d_inv)] = 0.\n",
    "        d_mat = sp.diags(d_inv)\n",
    "\n",
    "        norm_adj = d_mat.dot(adj_mat) # 유저가 시도한 문제 개수 제곱근으로 나눠주기\n",
    "        norm_adj = norm_adj.dot(self.d_mat) # 문제가 시도되어진 개수 제곱근으로 나눠주기\n",
    "        norm_adj = norm_adj.tocsr()\n",
    "\n",
    "        if self.split == True:\n",
    "            graph = self._split_A_hat(norm_adj)\n",
    "        else:\n",
    "            graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "            graph = graph.coalesce().to(self.device)\n",
    "        return graph\n",
    "    \n",
    "    def __build_test(self, test=True):\n",
    "        \"\"\"\n",
    "        return:\n",
    "            dict: {user: [items]}\n",
    "        \"\"\"        \n",
    "        \n",
    "        if test:\n",
    "            user_item_mat = self.test_user_item_mat\n",
    "            n = self.n_test_user\n",
    "        else:\n",
    "            user_item_mat = self.train_user_item_mat\n",
    "            n = self.n_user\n",
    "        \n",
    "        test_data = {}\n",
    "        for user in range(n):      \n",
    "            items = user_item_mat[user].nonzero()[0]\n",
    "            if len(items) >= self.maxK :\n",
    "                test_data[user] = items\n",
    "        \n",
    "        print(\"complete making test dict\")\n",
    "        \n",
    "        return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1344d40",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf3a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(BasicModel, self).__init__()\n",
    "    \n",
    "    def getUsersRating(self, users):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class PairWiseModel(BasicModel):\n",
    "    def __init__(self):\n",
    "        super(PairWiseModel, self).__init__()\n",
    "    def bpr_loss(self, users, pos, neg):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            users: users list \n",
    "            pos: positive items for corresponding users\n",
    "            neg: negative items for corresponding users\n",
    "        Return:\n",
    "            (log-loss, l2-loss)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class LightGCN(PairWiseModel):\n",
    "    def __init__(self, \n",
    "                 config:dict, \n",
    "                 dataset:BasicDataset):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.config = config\n",
    "        if self.config['dropout']:\n",
    "            self.keep_prob = self.config['keep_prob']\n",
    "        self.dataset : dataloader.BasicDataset = dataset\n",
    "        self.epoch = 0\n",
    "        self.__init_weight()\n",
    "\n",
    "    def __init_weight(self):\n",
    "        self.num_users  = self.dataset.n_users\n",
    "        self.num_items  = self.dataset.m_items\n",
    "        self.embedding_dim = self.config['embedding_dim']\n",
    "        self.n_layers = self.config['n_layers']\n",
    "        self.A_split = self.config['A_split']\n",
    "        self.embedding_user = torch.nn.Embedding(\n",
    "            self.num_users, self.embedding_dim)\n",
    "        self.embedding_item = torch.nn.Embedding(\n",
    "            self.num_items, self.embedding_dim)\n",
    "        if self.config['pretrain'] == 0:\n",
    "            nn.init.normal_(self.embedding_user.weight, std=0.1)\n",
    "            nn.init.normal_(self.embedding_item.weight, std=0.1)\n",
    "            print('use NORMAL distribution initilizer')\n",
    "        else:\n",
    "            self.embedding_user.weight.data.copy_(self.config['user_emb'])\n",
    "            self.embedding_item.weight.data.copy_(self.config['item_emb'])\n",
    "            print('use pretarined data')\n",
    "        self.f = nn.Sigmoid()\n",
    "        self.Graph = self.dataset.getSparseGraph()\n",
    "        print(f\"lgn is already to go(dropout:{self.config['dropout']})\")\n",
    "\n",
    "    def __dropout_x(self, x, keep_prob):\n",
    "        size = x.size()\n",
    "        index = x.indices().t()\n",
    "        values = x.values()\n",
    "        random_index = torch.rand(len(values)) + keep_prob\n",
    "        random_index = random_index.int().bool()\n",
    "        index = index[random_index]\n",
    "        values = values[random_index]/keep_prob\n",
    "        g = torch.sparse.FloatTensor(index.t(), values, size)\n",
    "        return g\n",
    "    \n",
    "    def __dropout(self, keep_prob):\n",
    "        if self.A_split:\n",
    "            graph = []\n",
    "            for g in self.Graph:\n",
    "                graph.append(self.__dropout_x(g, keep_prob))\n",
    "        else:\n",
    "            graph = self.__dropout_x(self.Graph, keep_prob)\n",
    "        return graph\n",
    "    \n",
    "    def computer(self):\n",
    "        \"\"\"\n",
    "        propagate methods for lightGCN\n",
    "        \"\"\"                    \n",
    "        users_emb = self.embedding_user.weight\n",
    "        items_emb = self.embedding_item.weight\n",
    "        \n",
    "        if self.config['dropout']:\n",
    "            #print(\"droping\")\n",
    "            graph = self.__dropout(self.keep_prob)\n",
    "        else:\n",
    "            graph = self.Graph   \n",
    "            \n",
    "        all_emb = torch.cat([users_emb, items_emb])\n",
    "        embs = [all_emb]\n",
    "        \n",
    "        \n",
    "        for layer in range(self.n_layers):\n",
    "            if self.A_split:\n",
    "                temp_emb = []\n",
    "                for f in range(len(graph)):\n",
    "                    temp_emb.append(torch.sparse.mm(graph[f], all_emb))\n",
    "                side_emb = torch.cat(temp_emb, dim=0)\n",
    "                all_emb = side_emb\n",
    "            else:\n",
    "                all_emb = torch.sparse.mm(graph, all_emb)\n",
    "            embs.append(all_emb)\n",
    "        embs = torch.stack(embs, dim=1)\n",
    "        #print(embs.size())\n",
    "        light_out = torch.mean(embs, dim=1)\n",
    "        users, items = torch.split(light_out, [self.num_users, self.num_items])\n",
    "        return users, items\n",
    "    \n",
    "    def getUsersRating(self, users):\n",
    "        all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users.long()]\n",
    "        items_emb = all_items\n",
    "        \n",
    "        rating = self.f(torch.matmul(users_emb, items_emb.t()))\n",
    "            \n",
    "        return rating\n",
    "    \n",
    "    def getEmbedding(self, users, pos_items, neg_items):\n",
    "        all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users]\n",
    "        pos_emb = all_items[pos_items]\n",
    "        neg_emb = all_items[neg_items]\n",
    "        users_emb_ego = self.embedding_user(users)\n",
    "        pos_emb_ego = self.embedding_item(pos_items)\n",
    "        neg_emb_ego = self.embedding_item(neg_items)\n",
    "        return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego\n",
    "    \n",
    "    def bpr_loss(self, users, pos, neg):\n",
    "        (users_emb, pos_emb, neg_emb, \n",
    "        userEmb0,  posEmb0, negEmb0) = self.getEmbedding(users.long(), pos.long(), neg.long())\n",
    "        reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
    "                         posEmb0.norm(2).pow(2)  +\n",
    "                         negEmb0.norm(2).pow(2))/float(len(users))\n",
    "        pos_scores = torch.mul(users_emb, pos_emb)\n",
    "        pos_scores = torch.sum(pos_scores, dim=1)\n",
    "        neg_scores = torch.mul(users_emb, neg_emb)\n",
    "        neg_scores = torch.sum(neg_scores, dim=1)\n",
    "        \n",
    "        loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
    "        \n",
    "        return loss, reg_loss\n",
    "       \n",
    "    def forward(self, users, items):\n",
    "        # compute embedding\n",
    "        all_users, all_items = self.computer()\n",
    "        # print('forward')\n",
    "        #all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users]\n",
    "        items_emb = all_items[items]\n",
    "        inner_pro = torch.mul(users_emb, items_emb)\n",
    "        gamma     = torch.sum(inner_pro, dim=1)\n",
    "        return gamma\n",
    "    \n",
    "class BPRLoss:\n",
    "    def __init__(self,\n",
    "                 recmodel : PairWiseModel,\n",
    "                 config : dict):\n",
    "        self.model = recmodel\n",
    "        self.weight_decay = config['decay']\n",
    "        self.lr = config['lr']\n",
    "        self.opt = torch.optim.Adam(recmodel.parameters(), lr=self.lr)\n",
    "\n",
    "    def stageOne(self, users, pos, neg):\n",
    "        loss, reg_loss = self.model.bpr_loss(users, pos, neg)\n",
    "        reg_loss = reg_loss*self.weight_decay\n",
    "        loss = loss + reg_loss\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f8194",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a7eb4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "CORES = multiprocessing.cpu_count() // 2\n",
    "\n",
    "#########################################\n",
    "################# Test ##################\n",
    "#########################################\n",
    "\n",
    "def getLabel(test_data, pred_data):\n",
    "    r = []\n",
    "    for i in range(len(test_data)):\n",
    "        groundTrue = test_data[i]\n",
    "        predictTopK = pred_data[i]\n",
    "        pred = list(map(lambda x: x in groundTrue, predictTopK))\n",
    "        pred = np.array(pred).astype(\"float\")\n",
    "        r.append(pred)\n",
    "    return np.array(r).astype('float')\n",
    "\n",
    "def NDCGatK_r(test_data,r,k):\n",
    "    \"\"\"\n",
    "    Normalized Discounted Cumulative Gain\n",
    "    rel_i = 1 or 0, so 2^{rel_i} - 1 = 1 or 0\n",
    "    \"\"\"\n",
    "    assert len(r) == len(test_data)\n",
    "    pred_data = r[:, :k]\n",
    "\n",
    "    test_matrix = np.zeros((len(pred_data), k))\n",
    "    for i, items in enumerate(test_data):\n",
    "        length = k if k <= len(items) else len(items)\n",
    "        test_matrix[i, :length] = 1\n",
    "    max_r = test_matrix\n",
    "    idcg = np.sum(max_r * 1./np.log2(np.arange(2, k + 2)), axis=1)\n",
    "    dcg = pred_data*(1./np.log2(np.arange(2, k + 2)))\n",
    "    dcg = np.sum(dcg, axis=1)\n",
    "    idcg[idcg == 0.] = 1.\n",
    "    ndcg = dcg/idcg\n",
    "    ndcg[np.isnan(ndcg)] = 0.\n",
    "    return np.sum(ndcg)\n",
    "\n",
    "def RecallPrecision_ATk(test_data, r, k):\n",
    "    \"\"\"\n",
    "    test_data should be a list? cause users may have different amount of pos items. shape (test_batch, k)\n",
    "    pred_data : shape (test_batch, k) NOTE: pred_data should be pre-sorted\n",
    "    k : top-k\n",
    "    \"\"\"\n",
    "    right_pred = r[:, :k].sum(1)\n",
    "    precis_n = k\n",
    "    recall_n = np.array([len(test_data[i]) for i in range(len(test_data))])\n",
    "    recall = np.sum(right_pred/recall_n)\n",
    "    precis = np.sum(right_pred)/precis_n\n",
    "    return {'recall': recall, 'precision': precis}\n",
    "\n",
    "\n",
    "def test_one_batch(X):\n",
    "    sorted_items = X[0].numpy()\n",
    "    groundTrue = X[1]\n",
    "    r = getLabel(groundTrue, sorted_items)\n",
    "    pre, recall, ndcg = [], [], []\n",
    "    for k in cfg['topks']:\n",
    "        ret = RecallPrecision_ATk(groundTrue, r, k)\n",
    "        pre.append(ret['precision'])\n",
    "        recall.append(ret['recall'])\n",
    "        ndcg.append(NDCGatK_r(groundTrue,r,k))\n",
    "    return {'recall':np.array(recall), \n",
    "            'precision':np.array(pre), \n",
    "            'ndcg':np.array(ndcg)}\n",
    "            \n",
    "def Test(dataset : BasicDataset, Recmodel : LightGCN, cfg):\n",
    "    u_batch_size = cfg['test_batch_size']\n",
    "    print('[----- Train Data -----]')\n",
    "    testDict: dict = dataset.trainDict\n",
    "    # eval mode with no dropout\n",
    "    Recmodel = Recmodel.eval()\n",
    "    max_K = max(cfg['topks'])\n",
    "    results = {'precision': np.zeros(len(cfg['topks'])),\n",
    "               'recall': np.zeros(len(cfg['topks'])),\n",
    "               'ndcg': np.zeros(len(cfg['topks']))}\n",
    "    with torch.no_grad():\n",
    "        users = list(testDict.keys())\n",
    "        try:\n",
    "            assert u_batch_size <= len(users) / 10\n",
    "        except AssertionError:\n",
    "            print(f\"test_u_batch_size is too big for this dataset, try a small one {len(users) // 10}\")\n",
    "        users_list = []\n",
    "        rating_list = []\n",
    "        groundTrue_list = []\n",
    "        # auc_record = []\n",
    "        # ratings = []\n",
    "        total_batch = len(users) // u_batch_size + 1\n",
    "        for batch_users in tqdm(minibatch(users, batch_size=u_batch_size)):\n",
    "            groundTrue = [testDict[u] for u in batch_users]\n",
    "            _batch_users = torch.Tensor(batch_users).long()\n",
    "            rating = Recmodel.getUsersRating(_batch_users)\n",
    "            #rating = rating.cpu()\n",
    "            print('Exclude Rated Item')\n",
    "            allPos = dataset.getUserPosItems(batch_users)\n",
    "            exclude_index = []\n",
    "            exclude_items = []\n",
    "            for range_i, items in enumerate(allPos):\n",
    "                exclude_index.extend([range_i] * len(items))\n",
    "                exclude_items.extend(items)\n",
    "            rating[exclude_index, exclude_items] = -(1<<10)\n",
    "            _, rating_K = torch.topk(rating, k=max_K)\n",
    "            rating = rating.cpu().numpy()\n",
    "            # aucs = [ \n",
    "            #         utils.AUC(rating[i],\n",
    "            #                   dataset, \n",
    "            #                   test_data) for i, test_data in enumerate(groundTrue)\n",
    "            #     ]\n",
    "            # auc_record.extend(aucs)\n",
    "            del rating\n",
    "            users_list.append(batch_users)\n",
    "            rating_list.append(rating_K.cpu())\n",
    "            groundTrue_list.append(groundTrue)\n",
    "        assert total_batch == len(users_list)\n",
    "        X = zip(rating_list, groundTrue_list)\n",
    "        pre_results = []\n",
    "        for x in X:\n",
    "            pre_results.append(test_one_batch(x))\n",
    "        scale = float(u_batch_size/len(users))\n",
    "        for result in pre_results:\n",
    "            results['recall'] += result['recall']\n",
    "            results['precision'] += result['precision']\n",
    "            results['ndcg'] += result['ndcg']\n",
    "        results['recall'] /= float(len(users))\n",
    "        results['precision'] /= float(len(users))\n",
    "        results['ndcg'] /= float(len(users))\n",
    "        # results['auc'] = np.mean(auc_record)\n",
    "        print(results)\n",
    "        return results\n",
    "\n",
    "#########################################\n",
    "################# TRAIN #################\n",
    "#########################################\n",
    "\n",
    "def minibatch(*tensors, **kwargs):\n",
    "\n",
    "    batch_size = kwargs.get('batch_size', cfg['batch_size'])\n",
    "\n",
    "    if len(tensors) == 1:\n",
    "        tensor = tensors[0]\n",
    "        for i in range(0, len(tensor), batch_size):\n",
    "            yield tensor[i:i + batch_size]\n",
    "    else:\n",
    "        for i in range(0, len(tensors[0]), batch_size):\n",
    "            yield tuple(x[i:i + batch_size] for x in tensors)\n",
    "\n",
    "def shuffle(*arrays, **kwargs):\n",
    "\n",
    "    require_indices = kwargs.get('indices', False)\n",
    "\n",
    "    if len(set(len(x) for x in arrays)) != 1:\n",
    "        raise ValueError('All inputs to shuffle must have '\n",
    "                         'the same length.')\n",
    "\n",
    "    shuffle_indices = np.arange(len(arrays[0]))\n",
    "    np.random.shuffle(shuffle_indices)\n",
    "\n",
    "    if len(arrays) == 1:\n",
    "        result = arrays[0][shuffle_indices]\n",
    "    else:\n",
    "        result = tuple(x[shuffle_indices] for x in arrays)\n",
    "\n",
    "    if require_indices:\n",
    "        return result, shuffle_indices\n",
    "    else:\n",
    "        return result \n",
    "\n",
    "def UniformSample_original_python(dataset : BasicDataset):\n",
    "    \"\"\"\n",
    "    the original impliment of BPR Sampling in LightGCN\n",
    "    :return:\n",
    "        np.array\n",
    "    \"\"\"\n",
    "    user_num = dataset.train_data_size\n",
    "    \n",
    "    users = np.random.randint(0, dataset.n_users, user_num)\n",
    "    allPos = dataset.allPos\n",
    "    S = []\n",
    "    for i, user in enumerate(users):\n",
    "        posForUser = allPos[user]\n",
    "        if len(posForUser) == 0:\n",
    "            continue\n",
    "        posindex = np.random.randint(0, len(posForUser))\n",
    "        positem = posForUser[posindex]\n",
    "        while True:\n",
    "            negitem = np.random.randint(0, dataset.m_items)\n",
    "            if negitem in posForUser:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        S.append([user, positem, negitem])\n",
    "    return np.array(S)\n",
    "    \n",
    "def BPR_train_original(dataset, recommend_model, loss_class : BPRLoss, cfg, neg_k=1):\n",
    "    Recmodel = recommend_model\n",
    "    Recmodel.train()\n",
    "    bpr = loss_class\n",
    "    \n",
    "    S = UniformSample_original_python(dataset)\n",
    "    users = torch.Tensor(S[:, 0]).long()\n",
    "    posItems = torch.Tensor(S[:, 1]).long()\n",
    "    negItems = torch.Tensor(S[:, 2]).long()\n",
    "\n",
    "    users = users.to(cfg['device'])\n",
    "    posItems = posItems.to(cfg['device'])\n",
    "    negItems = negItems.to(cfg['device'])\n",
    "    users, posItems, negItems = shuffle(users, posItems, negItems)\n",
    "    total_batch = len(users) // cfg['batch_size'] + 1\n",
    "    aver_loss = 0.\n",
    "    print(total_batch)\n",
    "    for (batch_i, (batch_users, batch_pos, batch_neg)) in tqdm(enumerate(minibatch(users, posItems, negItems,\n",
    "                                                                    batch_size=cfg['batch_size']))):\n",
    "        cri = bpr.stageOne(batch_users, batch_pos, batch_neg)\n",
    "        aver_loss += cri\n",
    "    aver_loss = aver_loss / total_batch\n",
    "    return f\"loss{aver_loss:.3f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d5305",
   "metadata": {},
   "source": [
    "## Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ea36034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init dataset\n",
      "make testDict\n",
      "complete making test dict\n",
      "complete making test dict\n",
      "train data size : 497076\n",
      "test user size : 750\n",
      "use NORMAL distribution initilizer\n",
      "loading adjacency matrix\n",
      "successfully loaded...\n",
      "don't split the matrix\n",
      "lgn is already to go(dropout:False)\n"
     ]
    }
   ],
   "source": [
    "# declare dataset, model, loss func\n",
    "dataset = BOJDataSet(cfg)\n",
    "Light_GCN = LightGCN(cfg, dataset).cuda()\n",
    "bpr_loss = BPRLoss(Light_GCN, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "128ba960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test]\n",
      "[----- Test Data -----]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "addmm: Argument #3 (dense): Expected dim 0 size 28732, got 32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Test]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mTest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLight_GCN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# test data로 평가\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     Test(dataset, Light_GCN, cfg, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# train data로 평가\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(Light_GCN\u001b[38;5;241m.\u001b[39mstate_dict(), cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_file_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[16], line 96\u001b[0m, in \u001b[0;36mTest\u001b[1;34m(dataset, Recmodel, cfg, test)\u001b[0m\n\u001b[0;32m     94\u001b[0m             groundTrue \u001b[38;5;241m=\u001b[39m [testDict[u] \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m batch_users]\n\u001b[0;32m     95\u001b[0m             _batch_users \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(batch_users)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m---> 96\u001b[0m             rating \u001b[38;5;241m=\u001b[39m \u001b[43mRecmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetUsersRating\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_batch_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m             \u001b[38;5;66;03m#rating = rating.cpu()\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m#             print('Exclude Rated Item')\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m#             allPos = dataset.getUserPosItems(batch_users)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m#                 exclude_items.extend(items)\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m#             rating[exclude_index, exclude_items] = -(1<<10)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m             _, rating_K \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(rating, k\u001b[38;5;241m=\u001b[39mmax_K)\n",
      "Cell \u001b[1;32mIn[15], line 118\u001b[0m, in \u001b[0;36mLightGCN.getUsersRating\u001b[1;34m(self, users, test)\u001b[0m\n\u001b[0;32m    116\u001b[0m     items_emb \u001b[38;5;241m=\u001b[39m all_items\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     users_emb, items_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputer\u001b[49m\u001b[43m(\u001b[49m\u001b[43musers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m rating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(torch\u001b[38;5;241m.\u001b[39mmatmul(users_emb, items_emb\u001b[38;5;241m.\u001b[39mt()))\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rating\n",
      "Cell \u001b[1;32mIn[15], line 104\u001b[0m, in \u001b[0;36mLightGCN.computer\u001b[1;34m(self, users, test)\u001b[0m\n\u001b[0;32m    102\u001b[0m         all_emb \u001b[38;5;241m=\u001b[39m side_emb\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m         all_emb \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     embs\u001b[38;5;241m.\u001b[39mappend(all_emb)\n\u001b[0;32m    106\u001b[0m embs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(embs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: addmm: Argument #3 (dense): Expected dim 0 size 28732, got 32"
     ]
    }
   ],
   "source": [
    "# Train & Test\n",
    "st_epoch = 0\n",
    "TRAIN_epochs = 100\n",
    "start = time.time()\n",
    "for epoch in range(st_epoch, st_epoch + TRAIN_epochs):\n",
    "    if epoch % 10 == 0:\n",
    "        print('[Test]')\n",
    "        Test(dataset, Light_GCN, cfg) # test data로 평가\n",
    "        Test(dataset, Light_GCN, cfg, test=False) # train data로 평가\n",
    "        torch.save(Light_GCN.state_dict(), cfg['model_file_path'])\n",
    "    \n",
    "    output_information = BPR_train_original(dataset, Light_GCN, bpr_loss, cfg)\n",
    "    print(f'EPOCH[{epoch+1}/{st_epoch + TRAIN_epochs}] {output_information}')\n",
    "torch.save(Light_GCN.state_dict(), cfg['model_file_path']+f\"_{st_epoch + TRAIN_epochs}\")\n",
    "total_time = time.time() - start\n",
    "print(f'total_time : {total_time}, loss : {output_information}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "79a5743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----- Test Data -----]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:01, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': array([0.85546667, 0.84113333]), 'recall': array([0.03442167, 0.06760341]), 'ndcg': array([0.8691201 , 0.85409283])}\n",
      "[----- Train Data -----]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:00, 69.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': array([0.87508571, 0.85405714]), 'recall': array([0.03412683, 0.06642766]), 'ndcg': array([0.8867245, 0.8678895])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': array([0.87508571, 0.85405714]),\n",
       " 'recall': array([0.03412683, 0.06642766]),\n",
       " 'ndcg': array([0.8867245, 0.8678895])}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "Test(dataset, Light_GCN, cfg)\n",
    "Test(dataset, Light_GCN, cfg, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "752e5bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:01, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': array([0.87666667, 0.85413333]), 'recall': array([0.0353388 , 0.06870723]), 'ndcg': array([0.88961997, 0.86918327])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': array([0.87666667, 0.85413333]),\n",
       " 'recall': array([0.0353388 , 0.06870723]),\n",
       " 'ndcg': array([0.88961997, 0.86918327])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test(dataset, Light_GCN, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5edc9453",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1314] 클라이언트가 필요한 권한을 가지고 있지 않습니다: './saved_model/lgcn_weight_10.pt' -> './saved_model\\\\lgcn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mislink(model_link_path):\n\u001b[0;32m     10\u001b[0m     os\u001b[38;5;241m.\u001b[39munlink(model_link_path)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_file_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_link_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m os\u001b[38;5;241m.\u001b[39mchmod(model_link_path, \u001b[38;5;241m0o777\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1314] 클라이언트가 필요한 권한을 가지고 있지 않습니다: './saved_model/lgcn_weight_10.pt' -> './saved_model\\\\lgcn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "lgcn_model_dir = './saved_model'\n",
    "lgcn_model_file_name = 'lgcn'\n",
    "model_link_path = os.path.join(lgcn_model_dir,lgcn_model_file_name)\n",
    "\n",
    "os.chmod(cfg['model_file_path'], 0o777)\n",
    "\n",
    "with open(cfg['model_file_path'], 'wb') as file:  # 파일을 바이너리 쓰기 모드(wb)로 열기\n",
    "    if os.path.islink(model_link_path):\n",
    "        os.unlink(model_link_path)\n",
    "    os.symlink(cfg['model_file_path'], model_link_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88bac582",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Light_GCN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mLight_GCN\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Light_GCN' is not defined"
     ]
    }
   ],
   "source": [
    "Light_GCN.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
